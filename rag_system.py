import os
from pdf2image import convert_from_path
import pytesseract
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.chains.llm import LLMChain
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
from tqdm import tqdm
import re
from datetime import datetime
from typing import Optional, Dict, List
import json

# Load .env
load_dotenv()

# Set Tesseract path
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

# Poppler path and document folder
POPLER_PATH = r"C:\Users\vidit\AppData\Local\Microsoft\WinGet\Packages\oschwartz10612.Poppler_Microsoft.Winget.Source_8wekyb3d8bbwe\poppler-24.08.0\Library\bin"
PDF_FOLDER = "./Documents"

# Language configurations
LANGUAGES = {
    "en": {
        "name": "English",
        "code": "en",
        "date_patterns": [
            r"\b([A-Za-z]+ \d{1,2}, \d{4})\b",
            r"\b(\d{1,2}[-/]\d{1,2}[-/]\d{4})\b",
            r"\b(\d{4}[-/]\d{1,2}[-/]\d{1,2})\b"
        ],
        "date_formats": ["%B %d, %Y", "%m/%d/%Y", "%Y/%m/%d", "%d/%m/%Y"]
    },
    "hi": {
        "name": "‡§π‡§ø‡§Ç‡§¶‡•Ä (Hindi)",
        "code": "hi",
        "date_patterns": [
            r"\b([A-Za-z]+ \d{1,2}, \d{4})\b",
            r"\b(\d{1,2}[-/]\d{1,2}[-/]\d{4})\b",
            r"\b(\d{4}[-/]\d{1,2}[-/]\d{1,2})\b"
        ],
        "date_formats": ["%B %d, %Y", "%m/%d/%Y", "%Y/%m/%d", "%d/%m/%Y"]
    },
    "es": {
        "name": "Espa√±ol (Spanish)",
        "code": "es",
        "date_patterns": [
            r"\b([A-Za-z]+ \d{1,2}, \d{4})\b",
            r"\b(\d{1,2}[-/]\d{1,2}[-/]\d{4})\b",
            r"\b(\d{4}[-/]\d{1,2}[-/]\d{1,2})\b"
        ],
        "date_formats": ["%B %d, %Y", "%m/%d/%Y", "%Y/%m/%d", "%d/%m/%Y"]
    },
    "fr": {
        "name": "Fran√ßais (French)",
        "code": "fr",
        "date_patterns": [
            r"\b([A-Za-z]+ \d{1,2}, \d{4})\b",
            r"\b(\d{1,2}[-/]\d{1,2}[-/]\d{4})\b",
            r"\b(\d{4}[-/]\d{1,2}[-/]\d{1,2})\b"
        ],
        "date_formats": ["%B %d, %Y", "%m/%d/%Y", "%Y/%m/%d", "%d/%m/%Y"]
    },
    "de": {
        "name": "Deutsch (German)",
        "code": "de",
        "date_patterns": [
            r"\b([A-Za-z]+ \d{1,2}, \d{4})\b",
            r"\b(\d{1,2}[-/]\d{1,2}[-/]\d{4})\b",
            r"\b(\d{4}[-/]\d{1,2}[-/]\d{1,2})\b"
        ],
        "date_formats": ["%B %d, %Y", "%m/%d/%Y", "%Y/%m/%d", "%d/%m/%Y"]
    },
    "zh": {
        "name": "‰∏≠Êñá (Chinese)",
        "code": "zh",
        "date_patterns": [
            r"\b([A-Za-z]+ \d{1,2}, \d{4})\b",
            r"\b(\d{1,2}[-/]\d{1,2}[-/]\d{4})\b",
            r"\b(\d{4}[-/]\d{1,2}[-/]\d{1,2})\b"
        ],
        "date_formats": ["%B %d, %Y", "%m/%d/%Y", "%Y/%m/%d", "%d/%m/%Y"]
    },
    "ja": {
        "name": "Êó•Êú¨Ë™û (Japanese)",
        "code": "ja",
        "date_patterns": [
            r"\b([A-Za-z]+ \d{1,2}, \d{4})\b",
            r"\b(\d{1,2}[-/]\d{1,2}[-/]\d{4})\b",
            r"\b(\d{4}[-/]\d{1,2}[-/]\d{1,2})\b"
        ],
        "date_formats": ["%B %d, %Y", "%m/%d/%Y", "%Y/%m/%d", "%d/%m/%Y"]
    }
}

# UI Messages in different languages
UI_MESSAGES = {
    "en": {
        "extracting": "üîç Extracting text and creating vector store...",
        "choose_option": "üìò Choose an option:",
        "generate_timeline": "1. Generate Timeline",
        "ask_question": "2. Ask a Question (Chat with Memory)",
        "change_language": "3. Change Language",
        "exit": "4. Exit",
        "timeline_output": "üìÖ TIMELINE OUTPUT:",
        "question_prompt": "\nAsk your legal question (or type 'back' to return to menu): ",
        "exiting": "üëã Exiting.",
        "invalid_choice": "‚ùå Invalid choice. Try again.",
        "language_selection": "üåê Select your preferred language:",
        "current_language": "Current language",
        "no_info": "I don't have sufficient information to answer this based on the available documents.",
        "bot_prefix": "ü§ñ"
    },
    "hi": {
        "extracting": "üîç ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§®‡§ø‡§ï‡§æ‡§≤ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§µ‡•á‡§ï‡•ç‡§ü‡§∞ ‡§∏‡•ç‡§ü‡•ã‡§∞ ‡§¨‡§®‡§æ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç...",
        "choose_option": "üìò ‡§è‡§ï ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™ ‡§ö‡•Å‡§®‡•á‡§Ç:",
        "generate_timeline": "1. ‡§∏‡§Æ‡§Ø‡§∞‡•á‡§ñ‡§æ ‡§¨‡§®‡§æ‡§è‡§Ç",
        "ask_question": "2. ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§™‡•Ç‡§õ‡•á‡§Ç (‡§Æ‡•á‡§Æ‡•ã‡§∞‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ö‡•à‡§ü)",
        "change_language": "3. ‡§≠‡§æ‡§∑‡§æ ‡§¨‡§¶‡§≤‡•á‡§Ç",
        "exit": "4. ‡§¨‡§æ‡§π‡§∞ ‡§®‡§ø‡§ï‡§≤‡•á‡§Ç",
        "timeline_output": "üìÖ ‡§∏‡§Æ‡§Ø‡§∞‡•á‡§ñ‡§æ ‡§Ü‡§â‡§ü‡§™‡•Å‡§ü:",
        "question_prompt": "\n‡§Ö‡§™‡§®‡§æ ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§™‡•Ç‡§õ‡•á‡§Ç (‡§Ø‡§æ ‡§Æ‡•á‡§®‡•Ç ‡§™‡§∞ ‡§µ‡§æ‡§™‡§∏ ‡§ú‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è 'back' ‡§ü‡§æ‡§á‡§™ ‡§ï‡§∞‡•á‡§Ç): ",
        "exiting": "üëã ‡§¨‡§æ‡§π‡§∞ ‡§®‡§ø‡§ï‡§≤ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç‡•§",
        "invalid_choice": "‚ùå ‡§ó‡§≤‡§§ ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™‡•§ ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•á‡§Ç‡•§",
        "language_selection": "üåê ‡§Ö‡§™‡§®‡•Ä ‡§™‡§∏‡§Ç‡§¶‡•Ä‡§¶‡§æ ‡§≠‡§æ‡§∑‡§æ ‡§ö‡•Å‡§®‡•á‡§Ç:",
        "current_language": "‡§µ‡§∞‡•ç‡§§‡§Æ‡§æ‡§® ‡§≠‡§æ‡§∑‡§æ",
        "no_info": "‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡•ã‡§Ç ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ ‡§á‡§∏‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡•á‡§∞‡•á ‡§™‡§æ‡§∏ ‡§™‡§∞‡•ç‡§Ø‡§æ‡§™‡•ç‡§§ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§",
        "bot_prefix": "ü§ñ"
    },
    "es": {
        "extracting": "üîç Extrayendo texto y creando almac√©n vectorial...",
        "choose_option": "üìò Elige una opci√≥n:",
        "generate_timeline": "1. Generar Cronolog√≠a",
        "ask_question": "2. Hacer una Pregunta (Chat con Memoria)",
        "change_language": "3. Cambiar Idioma",
        "exit": "4. Salir",
        "timeline_output": "üìÖ SALIDA DE CRONOLOG√çA:",
        "question_prompt": "\nHaz tu pregunta legal (o escribe 'back' para volver al men√∫): ",
        "exiting": "üëã Saliendo.",
        "invalid_choice": "‚ùå Opci√≥n inv√°lida. Int√©ntalo de nuevo.",
        "language_selection": "üåê Selecciona tu idioma preferido:",
        "current_language": "Idioma actual",
        "no_info": "No tengo suficiente informaci√≥n para responder esto bas√°ndome en los documentos disponibles.",
        "bot_prefix": "ü§ñ"
    },
    "fr": {
        "extracting": "üîç Extraction de texte et cr√©ation du magasin vectoriel...",
        "choose_option": "üìò Choisissez une option:",
        "generate_timeline": "1. G√©n√©rer la Chronologie",
        "ask_question": "2. Poser une Question (Chat avec M√©moire)",
        "change_language": "3. Changer de Langue",
        "exit": "4. Quitter",
        "timeline_output": "üìÖ SORTIE DE CHRONOLOGIE:",
        "question_prompt": "\nPosez votre question juridique (ou tapez 'back' pour revenir au menu): ",
        "exiting": "üëã Sortie.",
        "invalid_choice": "‚ùå Choix invalide. R√©essayez.",
        "language_selection": "üåê S√©lectionnez votre langue pr√©f√©r√©e:",
        "current_language": "Langue actuelle",
        "no_info": "Je n'ai pas suffisamment d'informations pour r√©pondre √† cela bas√© sur les documents disponibles.",
        "bot_prefix": "ü§ñ"
    },
    "de": {
        "extracting": "üîç Text extrahieren und Vektorspeicher erstellen...",
        "choose_option": "üìò W√§hlen Sie eine Option:",
        "generate_timeline": "1. Zeitleiste generieren",
        "ask_question": "2. Eine Frage stellen (Chat mit Ged√§chtnis)",
        "change_language": "3. Sprache √§ndern",
        "exit": "4. Beenden",
        "timeline_output": "üìÖ ZEITLEISTEN-AUSGABE:",
        "question_prompt": "\nStellen Sie Ihre rechtliche Frage (oder geben Sie 'back' ein, um zum Men√º zur√ºckzukehren): ",
        "exiting": "üëã Beenden.",
        "invalid_choice": "‚ùå Ung√ºltige Wahl. Versuchen Sie es erneut.",
        "language_selection": "üåê W√§hlen Sie Ihre bevorzugte Sprache:",
        "current_language": "Aktuelle Sprache",
        "no_info": "Ich habe nicht gen√ºgend Informationen, um dies basierend auf den verf√ºgbaren Dokumenten zu beantworten.",
        "bot_prefix": "ü§ñ"
    },
    "zh": {
        "extracting": "üîç Ê≠£Âú®ÊèêÂèñÊñáÊú¨Âπ∂ÂàõÂª∫ÂêëÈáèÂ≠òÂÇ®...",
        "choose_option": "üìò ÈÄâÊã©‰∏Ä‰∏™ÈÄâÈ°π:",
        "generate_timeline": "1. ÁîüÊàêÊó∂Èó¥Á∫ø",
        "ask_question": "2. ÊèêÈóÆ (Â∏¶ËÆ∞ÂøÜÁöÑËÅäÂ§©)",
        "change_language": "3. Êõ¥ÊîπËØ≠Ë®Ä",
        "exit": "4. ÈÄÄÂá∫",
        "timeline_output": "üìÖ Êó∂Èó¥Á∫øËæìÂá∫:",
        "question_prompt": "\nÊèêÂá∫ÊÇ®ÁöÑÊ≥ïÂæãÈóÆÈ¢ò (ÊàñËæìÂÖ• 'back' ËøîÂõûËèúÂçï): ",
        "exiting": "üëã Ê≠£Âú®ÈÄÄÂá∫„ÄÇ",
        "invalid_choice": "‚ùå Êó†ÊïàÈÄâÊã©„ÄÇËØ∑ÈáçËØï„ÄÇ",
        "language_selection": "üåê ÈÄâÊã©ÊÇ®ÁöÑÈ¶ñÈÄâËØ≠Ë®Ä:",
        "current_language": "ÂΩìÂâçËØ≠Ë®Ä",
        "no_info": "Ê†πÊçÆÂèØÁî®ÊñáÊ°£ÔºåÊàëÊ≤°ÊúâË∂≥Â§üÁöÑ‰ø°ÊÅØÊù•ÂõûÁ≠îËøô‰∏™ÈóÆÈ¢ò„ÄÇ",
        "bot_prefix": "ü§ñ"
    },
    "ja": {
        "extracting": "üîç „ÉÜ„Ç≠„Çπ„Éà„ÇíÊäΩÂá∫„Åó„Å¶„Éô„ÇØ„Çø„Éº„Çπ„Éà„Ç¢„Çí‰ΩúÊàê‰∏≠...",
        "choose_option": "üìò „Ç™„Éó„Ç∑„Éß„É≥„ÇíÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑ:",
        "generate_timeline": "1. „Çø„Ç§„É†„É©„Ç§„É≥„ÇíÁîüÊàê",
        "ask_question": "2. Ë≥™Âïè„Åô„Çã („É°„É¢„É™‰ªò„Åç„ÉÅ„É£„ÉÉ„Éà)",
        "change_language": "3. Ë®ÄË™û„ÇíÂ§âÊõ¥",
        "exit": "4. ÁµÇ‰∫Ü",
        "timeline_output": "üìÖ „Çø„Ç§„É†„É©„Ç§„É≥Âá∫Âäõ:",
        "question_prompt": "\nÊ≥ïÁöÑË≥™Âïè„Çí„Åó„Å¶„Åè„Å†„Åï„ÅÑ ('back'„Å®ÂÖ•Âäõ„Åó„Å¶„É°„Éã„É•„Éº„Å´Êàª„Çã): ",
        "exiting": "üëã ÁµÇ‰∫Ü‰∏≠„ÄÇ",
        "invalid_choice": "‚ùå ÁÑ°Âäπ„Å™ÈÅ∏Êäû„Åß„Åô„ÄÇ„ÇÇ„ÅÜ‰∏ÄÂ∫¶Ë©¶„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
        "language_selection": "üåê „ÅäÂ•Ω„Åø„ÅÆË®ÄË™û„ÇíÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑ:",
        "current_language": "ÁèæÂú®„ÅÆË®ÄË™û",
        "no_info": "Âà©Áî®ÂèØËÉΩ„Å™ÊñáÊõ∏„Å´Âü∫„Å•„ÅÑ„Å¶„ÄÅ„Åì„Çå„Å´Á≠î„Åà„Çã„ÅÆ„Å´ÂçÅÂàÜ„Å™ÊÉÖÂ†±„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ",
        "bot_prefix": "ü§ñ"
    }
}

class MultilingualRAGSystem:
    def __init__(self):
        self.embeddings = OpenAIEmbeddings()
        self.vector_store: Optional[FAISS] = None
        self.model = ChatOpenAI(model="gpt-4o", temperature=0.7)
        self.memory: Optional[ConversationSummaryBufferMemory] = None
        self.qa_chain: Optional[ConversationalRetrievalChain] = None
        self.current_language = "en"  # Default language
        self.timeline_cache = None  # Cache timeline across language switches
        
    def set_language(self, language_code: str):
        """Set the current language for responses"""
        if language_code in LANGUAGES:
            self.current_language = language_code
            # Recreate QA chain with new language if it exists
            if self.qa_chain is not None:
                self.setup_conversational_query_system()
        else:
            raise ValueError(f"Language {language_code} not supported")

    def get_ui_message(self, key: str) -> str:
        """Get UI message in current language"""
        return UI_MESSAGES.get(self.current_language, UI_MESSAGES["en"]).get(key, key)

    def extract_text_from_pdfs(self, folder_path):
        all_docs = []
        for file in tqdm(os.listdir(folder_path)):
            if file.endswith(".pdf"):
                pdf_path = os.path.join(folder_path, file)
                try:
                    images = convert_from_path(pdf_path, poppler_path=POPLER_PATH)
                except Exception as e:
                    # Fallback: try without poppler_path if it fails
                    try:
                        images = convert_from_path(pdf_path)
                    except Exception as e2:
                        print(f"Error processing {file}: {e2}")
                        continue

                full_text = ""
                for img in images:
                    text = pytesseract.image_to_string(img, lang="hin+eng")
                    full_text += text.strip() + "\n"

                doc = Document(page_content=full_text, metadata={"source": file})
                # print("document", file)
                # print("content", full_text)
                all_docs.append(doc)
        return all_docs

    def chunk_documents(self, docs):
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        return splitter.split_documents(docs)

    def create_vector_store(self, chunks):
        self.vector_store = FAISS.from_documents(chunks, self.embeddings)
        self.vector_store.save_local("faiss_index")

    def load_vector_store(self):
        self.vector_store = FAISS.load_local(
            "faiss_index", self.embeddings, allow_dangerous_deserialization=True
        )

    def extract_date_advanced(self, entry: str) -> datetime:
        """Advanced date extraction supporting multiple patterns and languages"""
        lang_config = LANGUAGES.get(self.current_language, LANGUAGES["en"])
        
        for pattern in lang_config["date_patterns"]:
            match = re.search(pattern, entry)
            if match:
                date_str = match.group(1)
                for date_format in lang_config["date_formats"]:
                    try:
                        return datetime.strptime(date_str, date_format)
                    except ValueError:
                        continue
        
        # Additional fallback patterns
        fallback_patterns = [
            r"\b(\d{1,2}[./]\d{1,2}[./]\d{2,4})\b",
            r"\b(\d{4}[./]\d{1,2}[./]\d{1,2})\b",
            r"\b(\d{1,2}\s+[A-Za-z]+\s+\d{4})\b"
        ]
        
        for pattern in fallback_patterns:
            match = re.search(pattern, entry)
            if match:
                date_str = match.group(1)
                for fmt in ["%d/%m/%Y", "%m/%d/%Y", "%Y/%m/%d", "%d.%m.%Y", "%d %B %Y", "%d %b %Y"]:
                    try:
                        return datetime.strptime(date_str, fmt)
                    except ValueError:
                        continue
        
        return datetime.max  # Put undated entries at the end

    def generate_timeline(self, force_refresh: bool = False):
        if self.vector_store is None:
            raise ValueError("Vector store not initialized. Load or create it first.")

        # Use cached timeline if available and not forcing refresh
        if self.timeline_cache and not force_refresh:
            # Just translate the cached timeline to current language
            return self.translate_timeline_to_language(self.timeline_cache)

        results = self.vector_store.similarity_search(
            "timeline of legal events, orders, applications, case filings, affidavits, wills, replies, and judgments", k=130
        )
        context = "\n\n".join([doc.page_content[:1500] for doc in results])

        language_name = LANGUAGES[self.current_language]["name"]
        
        prompt = f"""
You are a legal assistant reviewing scanned documents (translated to text via OCR). 

Your task is to generate a **detailed, chronological timeline in {language_name}** of all events relevant to the documents.

‚úÖ For each entry:
- Extract the **full date** (day, month, year) if available
- Mention **the action taken**
- Mention **who did what**, against whom, and why
- Include any **case numbers**, **legal sections**, or **documents referenced**
- Respond ONLY in {language_name}

üìå Format strictly like this:
1. [Full Date] - [Actor] [Action] [Details and Legal References]

If responding in {language_name}, make sure ALL text including dates, names, and legal terms are properly expressed in {language_name}.

Documents:
{context}

Now, generate the full timeline in {language_name}:
"""
        print("PROMPT IS HERE")
        print(prompt)
        response = self.model.invoke(prompt)
        if hasattr(response, "content") and isinstance(response.content, str):
            text = response.content
        elif isinstance(response, str):
            text = response
        else:
            raise TypeError("Unexpected response type from model.invoke()")

        entries = [line.strip() for line in text.strip().split("\n") if re.match(r"^\d+\.\s", line)]

        # Sort entries chronologically using advanced date extraction
        sorted_entries = sorted(entries, key=self.extract_date_advanced)
        
        timeline_result = "\n".join(sorted_entries)
        
        # Cache the timeline
        self.timeline_cache = {
            "entries": sorted_entries,
            "context": context,
            "language": self.current_language,
            "full_text": timeline_result
        }
        
        return timeline_result, context

    def translate_timeline_to_language(self, cached_timeline):
        """Translate cached timeline to current language"""
        if cached_timeline["language"] == self.current_language:
            return cached_timeline["full_text"], cached_timeline["context"]
        
        language_name = LANGUAGES[self.current_language]["name"]
        
        prompt = f"""
Translate the following legal timeline to {language_name}. Maintain the exact same structure and chronological order. 
Make sure ALL content including dates, legal terms, and names are properly expressed in {language_name}.

Original Timeline:
{cached_timeline["full_text"]}

Translated Timeline in {language_name}:
"""
        
        response = self.model.invoke(prompt)
        if hasattr(response, "content") and isinstance(response.content, str):
            translated_text = response.content
        elif isinstance(response, str):
            translated_text = response
        else:
            translated_text = cached_timeline["full_text"]
        
        return translated_text, cached_timeline["context"]

    def setup_conversational_query_system(self):
        if self.vector_store is None:
            raise ValueError("Vector store not initialized. Load or create it first.")

        retriever = self.vector_store.as_retriever(search_kwargs={"k": 15})
        
        # Preserve existing memory if it exists
        if self.memory is None:
            self.memory = ConversationSummaryBufferMemory(
                llm=self.model,
                memory_key="chat_history",
                return_messages=True
            )

        language_name = LANGUAGES[self.current_language]["name"]
        no_info_message = self.get_ui_message("no_info")

        prompt_template = """You are a helpful legal assistant. You answer user queries about a civil property case based on documents.

IMPORTANT: Respond ONLY in {language_name}. All your responses must be in {language_name}.

‚úÖ Use the provided context to answer the user's question, even if the documents are in different languages (Hindi, English, etc.).
‚úÖ Extract relevant information from the context and provide a comprehensive response in **{language_name}**.
‚úÖ If you can find ANY relevant information in the context about the legal case, use it to answer the question.
‚úÖ Only say "{no_info_message}" if the context contains absolutely NO relevant information about the user's question.
‚úÖ For general questions not about the legal case, use your general knowledge and respond in {language_name}.

Context from legal documents:
{{context}}

Chat History:
{{chat_history}}

User Question:
{{question}}

Detailed Answer (in {language_name}):""".format(language_name=language_name, no_info_message=no_info_message)

        PROMPT = PromptTemplate(
            input_variables=["context", "chat_history", "question"],
            template=prompt_template
        )

        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.model,
            retriever=retriever,
            memory=self.memory,
            combine_docs_chain_kwargs={"prompt": PROMPT}
        )

    def ask_question(self, user_question):
        if self.qa_chain is None:
            raise ValueError("Conversational query system not set up. Call setup_conversational_query_system() first.")
        if self.vector_store is None:
            raise ValueError("Vector store not initialized. Load or create it first.")
        
        # Test retrieval first to debug
        retriever = self.vector_store.as_retriever(search_kwargs={"k": 5})
        docs = retriever.get_relevant_documents(user_question)
        # print(f"\nüîç Found {len(docs)} relevant documents for your question.")
        
        if len(docs) == 0:
            return self.get_ui_message("no_info")
            
        # Use invoke instead of deprecated run method
        try:
            response = self.qa_chain.invoke({"question": user_question})
            return response.get("answer", "No response generated")
        except Exception as e:
            # Fallback: simple retrieval + direct model response
            return self.fallback_answer(user_question, docs)
    
    def fallback_answer(self, user_question, docs):
        """Fallback method if ConversationalRetrievalChain fails"""
        context = "\n\n".join([doc.page_content[:1000] for doc in docs])
        language_name = LANGUAGES[self.current_language]["name"]
        
        prompt = f"""You are a helpful legal assistant. Answer the user's question based on the provided context from legal documents.

IMPORTANT: Respond ONLY in {language_name}.

The context may contain text in different languages (Hindi, English, etc.). Extract and use ALL relevant information to provide a comprehensive answer.

Context from legal documents:
{context}

User Question: {user_question}

Based on the legal documents provided above, here is my detailed answer in {language_name}:"""

        response = self.model.invoke(prompt)
        if hasattr(response, "content"):
            return response.content
        return str(response)

def select_language() -> str:
    """Language selection interface"""
    print("\nüåê Select your preferred language / ‡§Ö‡§™‡§®‡•Ä ‡§™‡§∏‡§Ç‡§¶‡•Ä‡§¶‡§æ ‡§≠‡§æ‡§∑‡§æ ‡§ö‡•Å‡§®‡•á‡§Ç / Selecciona tu idioma preferido:")
    print("=" * 60)
    
    for i, (code, lang_info) in enumerate(LANGUAGES.items(), 1):
        print(f"{i}. {lang_info['name']}")
    
    while True:
        try:
            choice = input(f"\nEnter choice (1-{len(LANGUAGES)}): ").strip()
            choice_num = int(choice)
            if 1 <= choice_num <= len(LANGUAGES):
                return list(LANGUAGES.keys())[choice_num - 1]
            else:
                print(f"Please enter a number between 1 and {len(LANGUAGES)}")
        except ValueError:
            print("Please enter a valid number")

def change_language_menu(rag_system: MultilingualRAGSystem) -> bool:
    """Language change interface"""
    current_lang = LANGUAGES[rag_system.current_language]["name"]
    print(f"\n{rag_system.get_ui_message('current_language')}: {current_lang}")
    print(rag_system.get_ui_message('language_selection'))
    print("=" * 60)
    
    for i, (code, lang_info) in enumerate(LANGUAGES.items(), 1):
        marker = " ‚úì" if code == rag_system.current_language else ""
        print(f"{i}. {lang_info['name']}{marker}")
    
    print(f"{len(LANGUAGES) + 1}. Back to main menu")
    
    while True:
        try:
            choice = input(f"\nEnter choice (1-{len(LANGUAGES) + 1}): ").strip()
            choice_num = int(choice)
            if 1 <= choice_num <= len(LANGUAGES):
                new_lang = list(LANGUAGES.keys())[choice_num - 1]
                rag_system.set_language(new_lang)
                print(f"‚úÖ Language changed to {LANGUAGES[new_lang]['name']}")
                return True
            elif choice_num == len(LANGUAGES) + 1:
                return False
            else:
                print(f"Please enter a number between 1 and {len(LANGUAGES) + 1}")
        except ValueError:
            print("Please enter a valid number")

def main():
    # Language selection at startup
    print("=" * 60)
    print("üèõÔ∏è  MULTILINGUAL LEGAL RAG SYSTEM  üèõÔ∏è")
    print("=" * 60)
    
    selected_language = select_language()
    rag = MultilingualRAGSystem()
    # rag.extract_text_from_pdfs(PDF_FOLDER)
    # return
    rag.set_language(selected_language)
    
    print(f"\n‚úÖ Language set to: {LANGUAGES[selected_language]['name']}")

    if not os.path.exists("faiss_index"):
        print(rag.get_ui_message("extracting"))
        docs = rag.extract_text_from_pdfs(PDF_FOLDER)
        chunks = rag.chunk_documents(docs)
        rag.create_vector_store(chunks)
    else:
        rag.load_vector_store()

    while True:
        print(f"\n{rag.get_ui_message('choose_option')}")
        print(rag.get_ui_message("generate_timeline"))
        print(rag.get_ui_message("ask_question"))
        print(rag.get_ui_message("change_language"))
        print(rag.get_ui_message("exit"))
        
        choice = input("Enter your choice: ").strip()

        if choice == "1":
            print(f"\n{rag.get_ui_message('timeline_output')}\n")
            try:
                timeline, timeline_context = rag.generate_timeline()
                print(timeline)

                if rag.qa_chain is None:
                    rag.setup_conversational_query_system()

                if rag.memory and hasattr(rag.memory, "chat_memory"):
                    rag.memory.chat_memory.add_user_message("Generate timeline of the case.")
                    rag.memory.chat_memory.add_ai_message(timeline)
            except Exception as e:
                print(f"Error generating timeline: {e}")

        elif choice == "2":
            if rag.qa_chain is None:
                rag.setup_conversational_query_system()

            while True:
                question = input(rag.get_ui_message("question_prompt"))
                if question.lower() == "back":
                    break
                try:
                    response = rag.ask_question(question)
                    print(f"\n{rag.get_ui_message('bot_prefix')} {response}")
                except Exception as e:
                    print(f"Error: {e}")

        elif choice == "3":
            change_language_menu(rag)

        elif choice == "4":
            print(rag.get_ui_message("exiting"))
            break

        else:
            print(rag.get_ui_message("invalid_choice"))

if __name__ == "__main__":
    main()
